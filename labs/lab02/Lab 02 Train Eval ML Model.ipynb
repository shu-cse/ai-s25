{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "946d8431",
      "metadata": {
        "id": "946d8431",
        "papermill": {
          "duration": 0.011395,
          "end_time": "2023-11-14T08:40:08.983629",
          "exception": false,
          "start_time": "2023-11-14T08:40:08.972234",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Lab 02 (remixed): Training and Evaluating a Decision Tree\n",
        "\n",
        "Answer the exercise questions.\n",
        "\n",
        "**Objectives**: After completing these exercises, you should be able to:\n",
        "\n",
        "* Identify the components of an ML annotation project\n",
        "* Clean some data\n",
        "* Code a decision tree\n",
        "\n",
        "Written by: Dr. Stephen Wu\n",
        "\n",
        "References: [Titanic on Kaggle](https://www.kaggle.com/competitions/titanic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d0c90a",
      "metadata": {
        "id": "b3d0c90a",
        "papermill": {
          "duration": 0.011496,
          "end_time": "2023-11-14T08:40:12.176161",
          "exception": false,
          "start_time": "2023-11-14T08:40:12.164665",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Setting up the environment and Titanic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421f8034",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-11-14T08:40:09.008601Z",
          "iopub.status.busy": "2023-11-14T08:40:09.007665Z",
          "iopub.status.idle": "2023-11-14T08:40:12.127558Z",
          "shell.execute_reply": "2023-11-14T08:40:12.126571Z"
        },
        "id": "421f8034",
        "papermill": {
          "duration": 3.135484,
          "end_time": "2023-11-14T08:40:12.130323",
          "exception": false,
          "start_time": "2023-11-14T08:40:08.994839",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "%pip install anytree\n",
        "from anytree import Node, RenderTree\n",
        "\n",
        "# Install scikit-learn if not already installed, and run\n",
        "%pip install scikit-learn\n",
        "import sklearn # machine learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a6f144",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "execution": {
          "iopub.execute_input": "2023-11-14T08:40:12.201141Z",
          "iopub.status.busy": "2023-11-14T08:40:12.200185Z",
          "iopub.status.idle": "2023-11-14T08:40:12.257442Z",
          "shell.execute_reply": "2023-11-14T08:40:12.256161Z"
        },
        "id": "c6a6f144",
        "outputId": "b044f895-4965-4fd5-eef7-9adfbe6b50a4",
        "papermill": {
          "duration": 0.072628,
          "end_time": "2023-11-14T08:40:12.260115",
          "exception": false,
          "start_time": "2023-11-14T08:40:12.187487",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get the Titanic data (originally from Kaggle)\n",
        "dl_train_url = \"../lab01/train.csv\"\n",
        "dl_test_url = \"../lab01/test.csv\"\n",
        "\n",
        "train_data = pd.read_csv(dl_train_url)\n",
        "test_data = pd.read_csv(dl_test_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82fafdd7",
      "metadata": {
        "id": "82fafdd7",
        "papermill": {
          "duration": 0.013759,
          "end_time": "2023-11-14T08:40:13.334097",
          "exception": false,
          "start_time": "2023-11-14T08:40:13.320338",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Exercise 2.1: Problem setup (GRADED)\n",
        "Mark the correct answer with an `X`.\n",
        "\n",
        "1. Eventually, we want to _predict_ whether people `Survived` or not. What kind of an ML problem will this be?\n",
        "\n",
        "```\n",
        "    a) Clustering\n",
        "    b) Classification\n",
        "    c) Regression\n",
        "    d) Generation\n",
        "```\n",
        "\n",
        "2. What's the difference between `train_data` and `test_data`? '\n",
        "\n",
        "```\n",
        "    a) `test_data` has extra variables that `train_data` doesn't\n",
        "    b) `test_data` lacks the output variable\n",
        "    c) `test_data` is used first to help the algorithm find patterns on real data\n",
        "    d) `train_data` is \n",
        "```\n",
        "\n",
        "3. The split between `train_data` and `test_data` in this problem enables you to do what kind of learning?\n",
        "\n",
        "```\n",
        "    a) Supervised learning\n",
        "    b) Unsupervised learning\n",
        "    c) Reinforcement learning\n",
        "    d) Representation learning\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b347724",
      "metadata": {},
      "source": [
        "### Exercise 2.2: Code interpretation (GRADED)\n",
        "Write a comment using `# <WRITE A COMMENT>` at the end of each line. Hint: Check the documentation for [`Dataframe.fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna) and [`Dataframe.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b181f12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NaN stands for \"Not a Number\" and is a common way to represent missing data\n",
        "# What if our machine learning algorithm doesn't know how to handle missing data?\n",
        "print(train_data.isna().sum())\n",
        "train_data = train_data.fillna(method='ffill')\n",
        "test_data = test_data.fillna(method='ffill')\n",
        "\n",
        "# Some variables aren't useful for prediction, or aren't easy to use\n",
        "train_data = train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1, errors='ignore')\n",
        "test_data = test_data.drop(['Name', 'Ticket', 'Cabin'], axis=1, errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ccbb3e",
      "metadata": {
        "id": "15ccbb3e",
        "papermill": {
          "duration": 0.013487,
          "end_time": "2023-11-14T08:40:13.790535",
          "exception": false,
          "start_time": "2023-11-14T08:40:13.777048",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Decision Trees (pt 1): Data to test the algorithm\n",
        "In class, we talked about decision trees. The pseudocode given (from the textbook AIMA 19.3) was as follows:\n",
        "\n",
        "**function** LEARN-DECISION-TREE(*examples*, _attributes_, _parent\\_examples_) **returns** a tree<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**if** _examples_ is empty **then return** PLURALITY-VALUE(_parent\\_examples_)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**else if** all _examples_ have the same classification **then return** the classification<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**else if** _attributes_ is empty **then return** PLURALITY-VALUE(_examples_)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**else**<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_A_ = argmax(IMPORTANCE(_a_, _examples_))<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_tree_ = a new decision tree with root test _A_<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for each** value _v_ of _A_ **do**<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_exs_ = {_e_ : _e_ in _examples_ **and** _e.A_ = _v_}<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_subtree_ = LEARN-DECISION-TREE(_exs_, _attributes_ - _A_, _examples_)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add a branch to _tree_ with label (_A_ = _v_) and subtree _subtree_<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**return** _tree_\n",
        "\n",
        "\n",
        "You will write this function in Python!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90053d8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Information Gain is the standard \"Importance\" for Decision Trees\n",
        "# Take these for granted for now\n",
        "def information_gain(attribute, examples):\n",
        "    return entropy(examples) - remainder(attribute, examples)\n",
        "\n",
        "def entropy(examples):\n",
        "    num_survived = len(examples[examples['Survived'] == 1])\n",
        "    num_died = len(examples[examples['Survived'] == 0])\n",
        "    total = len(examples)\n",
        "    if num_survived == 0 or num_died == 0:\n",
        "        return 0\n",
        "    p_survived = num_survived / total\n",
        "    p_died = num_died / total\n",
        "    return -p_survived * np.log2(p_survived) - p_died * np.log2(p_died)\n",
        "\n",
        "def remainder(attribute, examples):\n",
        "    total = len(examples)\n",
        "    remainder = 0\n",
        "    for value in examples[attribute].unique():\n",
        "        exs = examples[examples[attribute] == value]\n",
        "        remainder += len(exs) / total * entropy(exs)\n",
        "    return remainder\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89abb63",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install anytree\n",
        "from anytree import AnyNode, RenderTree\n",
        "\n",
        "root_node = AnyNode(id=\"root\", parent=None, best_attribute=\"Area\")\n",
        "left_branch = AnyNode(id=\"L\", parent=root_node, best_attribute=None)\n",
        "right_branch = AnyNode(id=\"R\", parent=root_node, best_attribute=\"Alt\")\n",
        "rightleft_branch = AnyNode(id=\"RL\", parent=right_branch, best_attribute=None)\n",
        "rightright_branch = AnyNode(id=\"RR\", parent=right_branch, best_attribute=None)\n",
        "print(RenderTree(root_node))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90472b3",
      "metadata": {},
      "source": [
        "## Exercise 2.3: Decision tree implementation (GRADED)\n",
        "Turn the pseudocode for LEARN-DECISION-TREE into a real Python function, calling the `information_gain()` function defined above.\n",
        "\n",
        "You also need to define `plurality_value()`, which should return a `0` if there are more 0s left in the `examples`, or a `1` if more 1s are left in the `examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55ef8ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plurality_value(examples):\n",
        "    return examples['Survived'].value_counts().idxmax()\n",
        "\n",
        "def learn_decision_tree(examples, attributes, parent_examples):\n",
        "    \"\"\"\n",
        "    Recursively chooses attributes, defining a decision tree\n",
        "    that best fits the examples given, based on information \n",
        "    gain.\n",
        "\n",
        "    Args:\n",
        "       examples (Dataframe): samples of Titanic dataset\n",
        "       attributes (list): column names that are not used yet\n",
        "       parent_examples (Dataframe): examples at parent node\n",
        "\n",
        "    Returns:\n",
        "        AnyNode: the root of the decision tree\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"DEBUG: Starting learn_decision_tree() with\\n\"\n",
        "    #         + f\"{len(examples)} examples\\n\"\n",
        "    #         + f\"{len(attributes)} attributes\")\n",
        "\n",
        "    # Case: leaf node with no examples (all in other branch)\n",
        "    if examples.empty:\n",
        "        classification = plurality_value(parent_examples)\n",
        "        leaf_node = AnyNode(\n",
        "            id=\"no_examples\", \n",
        "            label=classification,\n",
        "            num_examples=0)\n",
        "        # print(\"DEBUG: Finishing \", RenderTree(leaf_node))\n",
        "        return leaf_node\n",
        "\n",
        "    # Case: leaf node with uniform class\n",
        "    elif len(examples['Survived'].value_counts()) == 1:\n",
        "        classification = examples['Survived'].iloc[0]\n",
        "        leaf_node = AnyNode(\n",
        "            id=\"same_class\",\n",
        "            label=classification,\n",
        "            num_examples=len(examples))\n",
        "        # print(\"DEBUG: Finishing \", RenderTree(leaf_node))\n",
        "        return leaf_node\n",
        "\n",
        "    # Case: leaf node with no attributes (pick majority class)\n",
        "    elif len(attributes) == 0:\n",
        "        classification = plurality_value(examples)\n",
        "        leaf_node = AnyNode(\n",
        "            id=\"no_attributes\", \n",
        "            label=classification,\n",
        "            num_examples=len(examples))\n",
        "        # print(\"DEBUG: Finishing \", RenderTree(leaf_node))\n",
        "        return leaf_node\n",
        "\n",
        "    # Case: non-leaf node\n",
        "    else:\n",
        "        # Choose the best attribute to split on (argmax importance)\n",
        "        best_attribute = None\n",
        "        best_importance = -1\n",
        "        for attribute in attributes:\n",
        "            importance = information_gain(attribute, examples)\n",
        "            # print(f\"DEBUG: tried {attribute}, has importance {importance}\")\n",
        "            if importance > best_importance:\n",
        "                best_importance = importance\n",
        "                best_attribute = attribute\n",
        "\n",
        "        # Create a new decision tree node with the best attribute\n",
        "        tree = AnyNode(\n",
        "            id=best_attribute,\n",
        "            parent=None, # temporary\n",
        "            num_examples=len(examples))\n",
        "        # print(\"DEBUG: Creating \", RenderTree(tree))\n",
        "        \n",
        "        # Remove the best attribute so children don't use it\n",
        "        child_attributes = [a for a in attributes \\\n",
        "            if a != best_attribute]\n",
        "\n",
        "        # For each value of the best attribute, create a branch\n",
        "        for value in examples[best_attribute].unique():\n",
        "            exs = examples[examples[best_attribute] == value]\n",
        "            subtree = learn_decision_tree(exs, child_attributes, examples)\n",
        "            subtree.parent = tree\n",
        "            subtree.parent_value = value\n",
        "\n",
        "        return tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8d2330",
      "metadata": {},
      "outputs": [],
      "source": [
        "mini_data = train_data.head(50)\n",
        "\n",
        "parent_examples_A = mini_data[mini_data['Sex'] == 'female']\n",
        "examples_A = mini_data[\n",
        "    ((mini_data['Sex'] == 'female') & mini_data['Survived'] == 1)]\n",
        "examples_B = mini_data[mini_data['Age'] > 40]\n",
        "examples_C = mini_data[mini_data['Age'] > 90]\n",
        "\n",
        "testable_attributes = mini_data.columns.unique().to_list()\n",
        "testable_attributes.remove('Survived')\n",
        "testable_attributes.remove('PassengerId')\n",
        "testable_attributes.remove('Fare')\n",
        "\n",
        "binned_data = train_data.copy()\n",
        "binned_data['Age'] = pd.cut(binned_data['Age'], bins=5)\n",
        "print(binned_data['Age'].value_counts())\n",
        "\n",
        "decision_tree = learn_decision_tree(\n",
        "    binned_data, testable_attributes, binned_data)\n",
        "print(RenderTree(decision_tree))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e84641",
      "metadata": {},
      "source": [
        "Now, let's test our code on a very simple data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be56d62",
      "metadata": {},
      "outputs": [],
      "source": [
        "example_data = pd.DataFrame({\n",
        "    'A1': [1, 1, 0, 1, 1],\n",
        "    'A2': [0, 0, 1, 1, 1],\n",
        "    'A3': [0, 1, 0, 1, 0],\n",
        "    'Survived' : [0, 0, 0, 1, 1]})\n",
        "\n",
        "print(learn_decision_tree(example_data, example_data, None))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5609d44d",
      "metadata": {},
      "source": [
        "What we're really after, though, is a decision tree that will be learned on the training set. Show what decision tree this `information_gain` importance function will produce, given this set of data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630558cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "output = learn_decision_tree(train_data, train_data.columns[:-1], None)\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 65.391415,
      "end_time": "2023-11-14T08:41:09.560267",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-11-14T08:40:04.168852",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
