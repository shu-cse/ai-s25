{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "946d8431",
      "metadata": {
        "id": "946d8431",
        "papermill": {
          "duration": 0.011395,
          "end_time": "2023-11-14T08:40:08.983629",
          "exception": false,
          "start_time": "2023-11-14T08:40:08.972234",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Lab 02 (remixed): Training and Evaluating a Decision Tree\n",
        "\n",
        "Answer the exercise questions.\n",
        "\n",
        "**Objectives**: After completing these exercises, you should be able to:\n",
        "\n",
        "* Identify the components of an ML annotation project\n",
        "* Clean some data\n",
        "* Code a decision tree\n",
        "\n",
        "Written by: Dr. Stephen Wu\n",
        "\n",
        "References: [Titanic on Kaggle](https://www.kaggle.com/competitions/titanic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d0c90a",
      "metadata": {
        "id": "b3d0c90a",
        "papermill": {
          "duration": 0.011496,
          "end_time": "2023-11-14T08:40:12.176161",
          "exception": false,
          "start_time": "2023-11-14T08:40:12.164665",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Setting up the environment and Titanic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421f8034",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-11-14T08:40:09.008601Z",
          "iopub.status.busy": "2023-11-14T08:40:09.007665Z",
          "iopub.status.idle": "2023-11-14T08:40:12.127558Z",
          "shell.execute_reply": "2023-11-14T08:40:12.126571Z"
        },
        "id": "421f8034",
        "papermill": {
          "duration": 3.135484,
          "end_time": "2023-11-14T08:40:12.130323",
          "exception": false,
          "start_time": "2023-11-14T08:40:08.994839",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Install scikit-learn if not already installed, and run\n",
        "%pip install scikit-learn\n",
        "import sklearn # machine learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a6f144",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "execution": {
          "iopub.execute_input": "2023-11-14T08:40:12.201141Z",
          "iopub.status.busy": "2023-11-14T08:40:12.200185Z",
          "iopub.status.idle": "2023-11-14T08:40:12.257442Z",
          "shell.execute_reply": "2023-11-14T08:40:12.256161Z"
        },
        "id": "c6a6f144",
        "outputId": "b044f895-4965-4fd5-eef7-9adfbe6b50a4",
        "papermill": {
          "duration": 0.072628,
          "end_time": "2023-11-14T08:40:12.260115",
          "exception": false,
          "start_time": "2023-11-14T08:40:12.187487",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get the Titanic data (originally from Kaggle)\n",
        "dl_train_url = \"../lab01/train.csv\"\n",
        "dl_test_url = \"../lab01/test.csv\"\n",
        "\n",
        "train_data = pd.read_csv(dl_train_url)\n",
        "test_data = pd.read_csv(dl_test_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82fafdd7",
      "metadata": {
        "id": "82fafdd7",
        "papermill": {
          "duration": 0.013759,
          "end_time": "2023-11-14T08:40:13.334097",
          "exception": false,
          "start_time": "2023-11-14T08:40:13.320338",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Exercise 2.1: Problem setup (GRADED)\n",
        "Mark the correct answer with an `X`.\n",
        "\n",
        "1. Eventually, we want to _predict_ whether people `Survived` or not. What kind of an ML problem will this be?\n",
        "\n",
        "```\n",
        "    a) Clustering\n",
        "    b) Classification\n",
        "    c) Regression\n",
        "    d) Generation\n",
        "```\n",
        "\n",
        "2. What's the difference between `train_data` and `test_data`? '\n",
        "\n",
        "```\n",
        "    a) `test_data` has extra variables that `train_data` doesn't\n",
        "    b) `test_data` lacks the output variable\n",
        "    c) `test_data` is used first to help the algorithm find patterns on real data\n",
        "    d) `train_data` is \n",
        "```\n",
        "\n",
        "3. The split between `train_data` and `test_data` in this problem enables you to do what kind of learning?\n",
        "\n",
        "```\n",
        "    a) Supervised learning\n",
        "    b) Unsupervised learning\n",
        "    c) Reinforcement learning\n",
        "    d) Representation learning\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b347724",
      "metadata": {},
      "source": [
        "### Exercise 2.2: Code interpretation (GRADED)\n",
        "Write a comment using `# <WRITE A COMMENT>` at the end of each line. Hint: Check the documentation for [`Dataframe.fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna) and [`Dataframe.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html#pandas.DataFrame.drop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b181f12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NaN stands for \"Not a Number\" and is a common way to represent missing data\n",
        "# What if our machine learning algorithm doesn't know how to handle missing data?\n",
        "print(train_data.isna().sum())\n",
        "train_data = train_data.fillna(method='ffill')\n",
        "test_data = test_data.fillna(method='ffill')\n",
        "\n",
        "# Some variables aren't useful for prediction, or aren't easy to use\n",
        "train_data = train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1, errors='ignore')\n",
        "test_data = test_data.drop(['Name', 'Ticket', 'Cabin'], axis=1, errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ccbb3e",
      "metadata": {
        "id": "15ccbb3e",
        "papermill": {
          "duration": 0.013487,
          "end_time": "2023-11-14T08:40:13.790535",
          "exception": false,
          "start_time": "2023-11-14T08:40:13.777048",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Decision Trees (pt 1): Data to test the algorithm\n",
        "In class, we talked about decision trees. The pseudocode given (from the textbook AIMA 19.3) was as follows:\n",
        "\n",
        "**function** LEARN-DECISION-TREE(*examples*, _attributes_, _parent\\_examples_) **returns** a tree<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**if** _examples_ is empty **then return** PLURALITY-VALUE(_parent\\_examples_)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**else if** all _examples_ have the same classification **then return** the classification<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**else if** _attributes_ is empty **then return** PLURALITY-VALUE(_examples_)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;**else**<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_A_ = argmax(IMPORTANCE(_a_, _examples_))<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_tree_ = a new decision tree with root test _A_<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for each** value _v_ of _A_ **do**<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_exs_ = {_e_ : _e_ in _examples_ **and** _e.A_ = _v_}<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_subtree_ = LEARN-DECISION-TREE(_exs_, _attributes_ - _A_, _examples_)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add a branch to _tree_ with label (_A_ = _v_) and subtree _subtree_<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**return** _tree_\n",
        "\n",
        "\n",
        "You will write this function in Python!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90053d8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Information Gain is the standard \"Importance\" for Decision Trees\n",
        "# Take these for granted for now\n",
        "def information_gain(attribute, examples):\n",
        "    return entropy(examples) - remainder(attribute, examples)\n",
        "\n",
        "def entropy(examples):\n",
        "    num_survived = len(examples[examples['Survived'] == 1])\n",
        "    num_died = len(examples[examples['Survived'] == 0])\n",
        "    total = len(examples)\n",
        "    if num_survived == 0 or num_died == 0:\n",
        "        return 0\n",
        "    p_survived = num_survived / total\n",
        "    p_died = num_died / total\n",
        "    return -p_survived * np.log2(p_survived) - p_died * np.log2(p_died)\n",
        "\n",
        "def remainder(attribute, examples):\n",
        "    total = len(examples)\n",
        "    remainder = 0\n",
        "    for value in examples[attribute].unique():\n",
        "        exs = examples[examples[attribute] == value]\n",
        "        remainder += len(exs) / total * entropy(exs)\n",
        "    return remainder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90472b3",
      "metadata": {},
      "source": [
        "## Exercise 2.3: Decision tree implementation (GRADED)\n",
        "Turn the pseudocode for LEARN-DECISION-TREE into a real Python function, calling the `information_gain()` function defined above.\n",
        "\n",
        "You also need to define `plurality_value()`, which should return a `0` if there are more 0s left in the `examples`, or a `1` if more 1s are left in the `examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55ef8ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plurality_value(examples):    \n",
        "# <YOUR CODE HERE>\n",
        "    return 0 # Placeholder\n",
        "\n",
        "def learn_decision_tree(examples, attributes, parent_examples):\n",
        "# <YOUR CODE HERE>\n",
        "    return {\"no attribute\" : {}} # Placeholder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e84641",
      "metadata": {},
      "source": [
        "Now, let's test our code. At any given place in the tree (node), one of these 4 cases applies:\n",
        "\n",
        "1. Examples at this node are all 1 (survivors) or all 0 (non-survivors).\n",
        "2. Examples at this node are mixed between 1 and 0.\n",
        "3. There are no examples at this node.\n",
        "4. There are no attributes at this node.\n",
        "\n",
        "Below, let's mimic each of the 4 cases of data. There are 4 \"mini\" data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be56d62",
      "metadata": {},
      "outputs": [],
      "source": [
        "mini_data = train_data.head(10)\n",
        "\n",
        "# Option A\n",
        "parent_examples_A = mini_data[mini_data['Sex'] == 'female']\n",
        "examples_A = mini_data[\n",
        "    ((mini_data['Sex'] == 'female') & mini_data['Survived'] == 1)]\n",
        "\n",
        "# Option B\n",
        "examples_B = mini_data[mini_data['Age'] > 40]\n",
        "\n",
        "# Option C\n",
        "examples_C = mini_data[mini_data['Age'] > 90]\n",
        "\n",
        "print(\"A\")\n",
        "print(examples_A)\n",
        "print(\"B\")\n",
        "print(examples_B)\n",
        "print(\"C\")\n",
        "print(examples_C)\n",
        "\n",
        "print(learn_decision_tree(mini_data, mini_data.columns[:-1], None))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5609d44d",
      "metadata": {},
      "source": [
        "What we're really after, though, is a decision tree that will be learned on the training set. Show what decision tree this `information_gain` importance function will produce, given this set of data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630558cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "output = learn_decision_tree(train_data, train_data.columns[:-1], None)\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 65.391415,
      "end_time": "2023-11-14T08:41:09.560267",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-11-14T08:40:04.168852",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
