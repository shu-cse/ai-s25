{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to Tibyan AI's **Launch Into Machine Learning** training workshop. This Jupyter Notebook is an interactive companion to the slides provided during the lectures."
      ],
      "metadata": {
        "id": "j-FovUNww-qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SESSION 1: DATA-CENTRIC AI\n",
        "After completing these exercises, at the end of Day 1, you should be able to:\n",
        "\n",
        "* Load tabular data into a `pandas` dataframe\n",
        "* Manipulate and organize the data via `pandas` dataframe filtering and indexing\n",
        "* Visualize the data in `seaborn`\n",
        "\n",
        "References: [Titanic on Kaggle](https://www.kaggle.com/competitions/titanic)"
      ],
      "metadata": {
        "id": "mUfLBe5ywnvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Import libraries for data exploration and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "OIoKExe8Zb62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.1: Collect & Visualize Data"
      ],
      "metadata": {
        "id": "gZNFZhohzRbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Titanic data\n",
        "\n",
        "We will load a dataset about who survived the Titanic disaster."
      ],
      "metadata": {
        "id": "9EX-e1a-9FQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run bash command line in Jupyter Notebooks with \"!\"\" to see the raw csv file\n",
        "!head titanic_data/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYxU0Cwplm_A",
        "outputId": "97adedc8-e870-4eaf-c419-090dd9a6cb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'titanic_data/train.csv' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of our analysis will be done in `pandas`. The [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function is full-featured and very useful."
      ],
      "metadata": {
        "id": "PxxZcI81k9FL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bmw8k2ewU5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "da8b7ff4-b518-499a-8590-7bb3a4279999"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic_data/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7e4ef043c1c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic_data/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m print(f\"The Dataframe (matrix) has {train_data.shape[0]} rows \"\n\u001b[1;32m      3\u001b[0m   + f\"and {train_data.shape[1]} columns.\\n\")\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Here's what some rows of data look like:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic_data/train.csv'"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv('titanic_data/train.csv')\n",
        "print(f\"The Dataframe (matrix) has {train_data.shape[0]} rows \"\n",
        "  + f\"and {train_data.shape[1]} columns.\\n\")\n",
        "print(\"Here's what some rows of data look like:\")\n",
        "display(train_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring data\n",
        "We've read our comma-separated value (CSV) file into a `pandas` [Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). In this case, `train_data` is a Dataframe that's shaped like a matrix. In `pandas` Dataframes, you can refer to things by their column (or row) headings as well as by index in the matrix."
      ],
      "metadata": {
        "id": "bLuIEcy_hoSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing & Selection. e.g., for item 2 of the 'Ticket' (8th) column\n",
        "print(f\"Referring by index, row 2 column 8:\\n{train_data.iloc[2][8]}\\n\")\n",
        "print(f\"Referring by 'Ticket' column, row 2:\\n{train_data['Ticket'][2]}\\n\")\n",
        "\n",
        "print(f\"Selecting a column by heading 'Ticket':\\n{train_data['Ticket']}\\n\")\n",
        "\n",
        "print(\"Selecting a row by the value in 'Ticket':\")\n",
        "print(train_data.loc[train_data['Ticket'] == 'STON/O2. 3101282'])"
      ],
      "metadata": {
        "id": "k7K4Wm3AiUi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BONUS: If all we're doing is visualizing, Google Colab has nice tools for data exploration. We'll load the \"Interactive Table\" feature in Google Colab to explore `pandas` Dataframes very quickly."
      ],
      "metadata": {
        "id": "rf5BmqwXgpZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "train_data"
      ],
      "metadata": {
        "id": "NmIH_VdjwXOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting a histogram with Seaborn\n",
        "Often, we'd like to visualize the data with specific plots. Let's try to plot the relationship between `Survived` and `Age` to see if older or younger people are more likely to survive, using a kind of bar chart called a histogram (see Seaborn documentation on [histplot](https://seaborn.pydata.org/generated/seaborn.histplot.html))."
      ],
      "metadata": {
        "id": "9OVWcd0D2pnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the original Age distribution\n",
        "sns.histplot(data=train_data, x=\"Age\", binwidth=10)"
      ],
      "metadata": {
        "id": "ybTVr8CM27aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Age distribution with survivor data overlaid\n",
        "sns.histplot(data=train_data, x=\"Age\", binwidth=10,\n",
        "             stat=\"probability\", multiple=\"fill\", hue=\"Survived\")"
      ],
      "metadata": {
        "id": "Xay48ccd8N9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 1.1\n",
        "Often, we want to programmatically calculate specific statistics. Here, let's explore the relationship between the `Surivived` and `Sex` variables in the dataset. Write code to calculate and print the following:\n",
        "\n",
        "* What percentage of the passengers\n",
        "survived the Titanic?\n",
        "* What percentage of the survivors were women? (Select survivors first)\n",
        "* What percentage of women were survivors? (Select women first)\n",
        "* Visualize the relationship between these two variables using a stacked histogram."
      ],
      "metadata": {
        "id": "YGZReok8j9oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 1.1(a)\n",
        "### What percentage of the passengers survived the Titanic?\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "print(f\"{}% of passengers survived the Titanic.\")\n",
        "\n",
        "### What percentage of the survivors were women? (Select survivors first)\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "print(f\"{}% of survivors were women.\")\n",
        "\n",
        "### What percentage of women were survivors? (Select women first)\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "print(f\"{}% of women were survivors.\")\n",
        "\n",
        "### Visualize the relationship b/t `Survived` and `Sex` via a stacked histogram\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "print(f\"This plot breaks down who 'Survived' according to 'Sex'\")\n"
      ],
      "metadata": {
        "id": "qOqnCF7RqTW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ### EXTRA: Flood disaster datasets\n",
        "Look at some data sets on flooding. Say that you would like to ...\n",
        "\n",
        "* Which of the [OpenFEMA](https://www.fema.gov/about/openfema/data-sets) data sets would you use if you were trying to find...\n",
        "* Access your chosen dataset [via API](https://www.fema.gov/about/openfema/api). Namely, use the `requests` library to send `https` commands and get results according to the API.\n",
        "* Pull a small amount of data to see its characteristics. -->\n"
      ],
      "metadata": {
        "id": "COkra01if8Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.2: Dataset cleaning\n",
        "In real life, data does not come in exactly the format you need for the problem you want to solve. You have to \"wrangle\" the data -- clean and transform it into something you can use. Let's try looking at some cryptocurrency (financial) data. (Adapted from Jongho Kim's introductory Jupyter Notebooks [1](https://github.com/jonghkim/financial-time-series-prediction-v2/blob/master/Module3/Hands-on-Labs/Lab_Orderbook_Data_Exploration.ipynb), [2](https://github.com/jonghkim/financial-time-series-prediction-v2/blob/master/Module2/Hands-on-Labs/Lab_Preprocessing_for_Cryptocurrency_Data.ipynb))"
      ],
      "metadata": {
        "id": "wRyn4PhNy16I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "uM5dMe5RydOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading cryptocurrency data\n",
        "We'll look at some data that is a bit more raw than the previous Titanic data. `coin_price` is financial data for several cryptocurrencies.\n"
      ],
      "metadata": {
        "id": "qBhGRjlnQDpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull data into pandas\n",
        "coin_price = pd.read_csv(\"/content/crypto_data/coin_price_dfs.csv\")\n",
        "print(f\"This dataframe is {coin_price.shape[0]} rows \" \\\n",
        "      + f\"and {coin_price.shape[1]} columns.\")\n",
        "\n",
        "orderbook = pd.read_csv(\"/content/crypto_data/orderbook_dfs.csv\")\n",
        "print(f\"This dataframe is {orderbook.shape[0]} rows \" \\\n",
        "      + f\"and {orderbook.shape[1]} columns.\")\n",
        "\n",
        "coin_price.head()"
      ],
      "metadata": {
        "id": "y4wWKkZb-LK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 1.2(a) Sorting by Time\n",
        "For both `coin_price` and `orderbook`:\n",
        "\n",
        "* There are 3 variables used to record time. Which among the three is inaccurate? (Hint: use `keys` to see headers; use `fromtimestamp` and `fromisoformat` from the [datetime](https://docs.python.org/3/library/datetime.html) library to convert between date formats).\n",
        "* Keep only one accurate column for time with the lowest storage cost. (Hint: use `drop` from [Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)).\n",
        "* Sort the data according to time. (Hint: use `sort_values` from [Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)).\n",
        "\n"
      ],
      "metadata": {
        "id": "jmpn51dukEGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_label = ''\n",
        "# <YOUR CODE HERE>\n",
        "\n",
        "print(f\"I am keeping the {time_label} column as my time\")"
      ],
      "metadata": {
        "id": "xSOfrf-mog4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting financial data\n",
        "After selecting rows only pertaining to Bitcoin, plot the `last` recorded price of the cryptocurrency at the end of the time frame."
      ],
      "metadata": {
        "id": "GdZXiUXExoCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select rows pertaining to currency_types of interest only\n",
        "currency_types = [\"btc\"]\n",
        "coin_price = coin_price[coin_price[\"currency\"].isin(currency_types)]\n",
        "\n",
        "g = sns.lineplot(\n",
        "    x=coin_price[time_label].apply(datetime.fromtimestamp),\n",
        "    y=coin_price['last'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TwdfPaPze3h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 1.2(b) Calculations and Missing values\n",
        "Data is not pristine in real life. Data collection may produce missing or incomplete data, even in highly automated settings.\n",
        "\n",
        "* How many missing values (`NaN`) are present in `coin_price`?  (Hint: use `isna` from [Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html))\n",
        "\n",
        "For `coin_price`, we calculate each timestamp's percentage change from the previous datapoint -- commonly used in stocks and save as a new `last_return` column.\n",
        "\n",
        "* Since this is a time series, fill in data on the `log_last_return` column for `NaN`s by using `interpolate` from [Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). This will estimate the value of a missing data point based on the values around it datapoint.\n",
        "* What happens if you do not fill in `NaN`s (or `dropna`, for example)?"
      ],
      "metadata": {
        "id": "Y2wrEjRr4JBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE>\n",
        "print(f\"There are {} NaNs in coin_price\")\n",
        "\n",
        "# Calculate the % change from 0 (no change) in log scale\n",
        "coin_price['last_return'] = coin_price['last'].pct_change()\n",
        "coin_price['log_last_return'] = np.log(1 + coin_price['last_return'])\n",
        "\n",
        "# <YOUR CODE HERE>\n",
        "print(f\"After interpolation: {coin_price.isna().sum()} NaNs remaining\")"
      ],
      "metadata": {
        "id": "C4RAIyo92dgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXTRA: Normalizing and plotting serial correlation\n",
        "Serial correlation is the relationship between a given variable and a lagged version of itself over various time intervals. In finance, this correlation is used by technical analysts to determine how well the past price of a security predicts the future price. Considering the `series` variable that we calculate below:\n",
        "\n",
        "* Plot the `log_last_return` using a `lag_plot` from the `pandas.plotting` library."
      ],
      "metadata": {
        "id": "2zPusw3t9fis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coin_price = coin_price.interpolate() # ANSWER FROM ABOVE\n",
        "series = (coin_price[\"log_last_return\"] - coin_price[\"log_last_return\"].mean())\\\n",
        "  / (coin_price[\"log_last_return\"])"
      ],
      "metadata": {
        "id": "IwoP5bMarE2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.lag_plot(series)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2T6rVG5U-dOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SESSION 2: ANNOTATION AND EVALUATION\n",
        "By the end of Session 2, you should be able to:\n",
        "\n",
        "* Annotate gold standard data for an ML task\n",
        "* Identify the components of an ML annotation project\n",
        "* Split data (a `dataframe`) for use in a ML project\n",
        "* Run a baseline ML algorithm on gold standard data"
      ],
      "metadata": {
        "id": "Avoc1k3YSPdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise 2.1: Annotation\n",
        "Here's a [list](https://www.simonwenkel.com/lists/software/list-of-annotation-tools-for-machine-learning-research.html) of annotation software that is open-source and mostly tailored to a specific data type, and a few like [Label Studio](https://labelstud.io/guide/get_started.html) and [Universal Data Tool](https://universaldatatool.com/) that give you the typical use cases.\n",
        "\n",
        "In this Exercise, you will use Label Studio to set up, annotate, and export an annotation project.\n",
        "\n",
        "We'll look at a subset of the MNIST dataset, which are images of handwritten numbers that are a standard tutorial tool for machine learning classification models. While we will use the dataset for ML algorithms later, right now the main thing is to understand what goes into creating ML datasets."
      ],
      "metadata": {
        "id": "nxrQYknD_-uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2.1(a): Install Label Studio\n",
        "Label Studio is a great tool but it needs to be installed locally on your machine.\n",
        "\n",
        "* Follow the Label Studio installation guide [here](https://labelstud.io/guide/install) (for reference, we have tried it via pip (which requires Python) and Docker) to install and start Label Studio. You are done with this when you have a browser open to http://localhost:8080/, welcoming you to Label Studio."
      ],
      "metadata": {
        "id": "yXPexJxKS8OT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2.1(b): Setup annotation project\n",
        "* Go to `Projects` -> `Create`\n",
        "  * Project Name: `MNIST_100 annotation`\n",
        "  * Data Import: `Upload Files` and select all images in `mnist_100` after you have downloaded them locally from [here](https://drive.google.com/drive/folders/1s3l7Tx1OrT3sl1eA_cpoUcZpnlerocRj?usp=drive_link). You should get a confirmation \"100 files uploaded.\"\n",
        "  * Labeling setup: `Computer Vision` -> `Image Classification` (should display a stock photo of airplanes)\n",
        "  * Save. You should get to a page where you are looking at rows of data.\n",
        "* Click `Settings`\n",
        "  * Select `General` -> `Task Sampling` -> `Random Sampling`\n",
        "  * Select `Labeling Interface` -> `Browse Templates` and `Visual`.\n",
        "  * Under `Add Choices` type the numbers 1-9 and 0, one per line, click `Add`. Click the red \"x\" next to \"Adult content\" and \"Weapons\" and \"Violence\". (The \"UI Preview\" should show `1[1]` and `2[2]` and `3[3]`, etc under the sample image.)\n",
        "  * Click `Save`."
      ],
      "metadata": {
        "id": "9K6VWPVmXO3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2.1(c): Label & Export data\n",
        "* Click on the Project Name at the top of the page to return to the rows of data.\n",
        "* Click on the `Label All Tasks` button.\n",
        "  * For each image, type or select the correct number, then click `Submit`. Do this for all 100 photos.\n",
        "* Click back on the Project Name, then select `Export` -> `JSON`. You should download a file like `project-1-at-nnnn.json`\n",
        "* Close Label Studio once you have validated that you have a real JSON file.\n"
      ],
      "metadata": {
        "id": "nTgCGmseXRsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2.1(d): Import data to Colab\n",
        "\n",
        "* Upload the JSON file to Colab's `Files`.\n",
        "* Upload the [`mnist_100` folder](https://drive.google.com/drive/folders/1s3l7Tx1OrT3sl1eA_cpoUcZpnlerocRj?usp=drive_link) to Colab.\n"
      ],
      "metadata": {
        "id": "9PF_wbUKXZCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra: Check your gold standard annotations\n",
        "Now that we have your gold standard labels on our data, let's read the data with [`pandas.read_json()`](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) and inspect it.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Fbqy3qck7er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'project-1.json'\n",
        "import json\n",
        "\n",
        "# Use `pandas.read_json` to bring the JSON data in\n",
        "with open('project-1.json', 'r') as json_file:\n",
        "  raw_data = json.load(json_file)\n",
        "\n",
        "# Look at the 0th row (i.e., data for one image)\n",
        "display(raw_data[0])"
      ],
      "metadata": {
        "id": "5GZCq7mthpeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is common for ML data such as these annotations to be buried in lots of messy places in a big JSON object.\n",
        "\n",
        "* Iterate through all of our images and pull at pieces of information that we actually care about, such as `'choices'` and `'file_upload'`. (Check out [Working with Large Nested JSON Data](https://ankushkunwar7777.medium.com/get-data-from-large-nested-json-file-cf1146aa8c9e\n",
        ").)"
      ],
      "metadata": {
        "id": "Wj6bXghL4suW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### NOT WORKING\n",
        "# Iterate through the JSON object and look for things of interest\n",
        "def extract_values(obj, key):\n",
        "    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n",
        "    arr = []\n",
        "\n",
        "    def extract(obj, arr, key):\n",
        "        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n",
        "        if isinstance(obj, dict):\n",
        "            for k, v in obj.items():\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    extract(v, arr, key)\n",
        "                elif k == key:\n",
        "                    arr.append(v)\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                extract(item, arr, key)\n",
        "        return arr\n",
        "\n",
        "    results = extract(obj, arr, key)\n",
        "    return results\n",
        "\n",
        "outcome = []\n",
        "for row in raw_data:\n",
        "    outcome.append(\n",
        "       {'id' : row['annotations'][0]['result']['id'],\n",
        "       'choice' : row['annotations'][0]['result'][0]['value']['choices'],\n",
        "       'file' : row['file_upload'] })\n",
        "# display(raw_data[0]['annotations'][0]['result']['id'])\n",
        "\n",
        "# mnist_data = pd.read_json(filename)\n",
        "# display(mnist_data.head())"
      ],
      "metadata": {
        "id": "FgX0csUY46yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2.2: Evaluation\n",
        "Before we even put a new ML model into place, we can evaluate how well a _baseline model_ works in the same setting. Our baseline model doesn't need to be complicated; this time we'll prepare all the data and use a simple algorithm called a Decision Tree (you'll learn more about it in Session 3).\n",
        "\n",
        "The Titanic data has a pre-defined train-test split -- `test.csv` being separate from `train.csv`. (In real life, we would need to create this split ourselves.) We will only evaluate `test_data` at the very end.\n"
      ],
      "metadata": {
        "id": "S9sgdSlnmr5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This training data\n",
        "train_data = pd.read_csv('titanic_data/train.csv')\n",
        "\n",
        "# Load held-out test data now, but don't use it until the end\n",
        "test_data = pd.read_csv('titanic_data/test.csv')\n",
        "\n",
        "def print_relative_size_of_datasets(df1, df2):\n",
        "  num_train_samples = df1.shape[0]\n",
        "  num_test_samples = df2.shape[0]\n",
        "  tot_samples = num_train_samples + num_test_samples\n",
        "  print(f\"The first dataset has {num_train_samples} samples \"\\\n",
        "        + f\"while the second dataset has {num_test_samples}; a \")\n",
        "  print(f\"{num_train_samples/tot_samples*100:.2f}\"\\\n",
        "        + f\"/{num_test_samples/tot_samples*100:.2f}\"\\\n",
        "        + f\" split of the data.\")\n",
        "\n",
        "print_relative_size_of_datasets(train_data, test_data)"
      ],
      "metadata": {
        "id": "WwrE5eZo2-lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a little quick-and-dirty processing to get rid of `NaNs` (it'd be best to consider each variable one at a time) and throw away some columns that probably won't help in the classification (they're completely unique for each passenger)."
      ],
      "metadata": {
        "id": "qAgck9Lc-z7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward-fill to eliminate NaNs\n",
        "train_data = train_data.fillna(method='ffill')\n",
        "test_data = test_data.fillna(method='ffill')\n",
        "\n",
        "# Drop some variables that probably won't be useful\n",
        "train_data = train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "test_data = test_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "display(train_data.head())"
      ],
      "metadata": {
        "id": "Eqh3JdpQ-x5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Splitting\n",
        "In the meantime, we'll split the training dataset again, into a training and development/validation set. (Remember that we need a validation set so that we can set _hyperparameters_ before running an algorithm on the test data.\n",
        "\n",
        "We'll use the classic ML library `sklearn` for a utility to help us do this."
      ],
      "metadata": {
        "id": "1n_pJYx93K2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Get indices for a split\n",
        "split = ShuffleSplit(n_splits = 1, test_size = 0.2)\n",
        "# Iterate through (only 1) split, setting train/val data\n",
        "for train_indices, test_indicices in split.split(train_data):\n",
        "    train_set = train_data.loc[train_indices]\n",
        "    val_set = train_data.loc[test_indicices]\n",
        "\n",
        "display(train_set.head())"
      ],
      "metadata": {
        "id": "LKQPUmsy3OhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2.2(a): Stratified sampling\n",
        "If, for example, your training data is 70% men and 30% women, but your test data is 80% women and 20% men, your ML model may not perform well on the test. This is called _sampling bias_. To help with this problem, we can try _stratifying_ the data according to variables of interest (e.g., `Sex`). This ensures both training and validation have similar distributions of the 'Survived', 'Pclass', and 'Sex' features for unbiased model evaluation.\n",
        "\n",
        "* Write an alternative split, stratifying the split according to 'Survived', 'PClass', and 'Sex'. Save the output as `strat_train_set` and `strat_val_set`. (Hint: pass in the columns you want to stratify via `y` in `StratifiedShuffleSplit`'s [`split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit.split).)\n",
        "* Verify that the percent of samples in each 'Pclass' value and `Sex` value are the same in `strat_train_set` and `strat_val_set`. (Hint: use Dataframe's `value_counts()` method.)"
      ],
      "metadata": {
        "id": "mf1EUDDLBfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HERE>\n",
        "\n",
        "##### SOLUTION\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2)\n",
        "for train_indices, test_indicices in split.split(\n",
        "    train_data, train_data[[\"Survived\" , \"Pclass\" , \"Sex\"]]):\n",
        "    strat_train_set = train_data.loc[train_indices]\n",
        "    strat_val_set = train_data.loc[test_indicices]\n",
        "\n",
        "display(strat_train_set['Pclass'].value_counts() / strat_train_set.shape[0] * 100)\n",
        "display(strat_val_set['Pclass'].value_counts() / strat_val_set.shape[0] * 100)"
      ],
      "metadata": {
        "id": "tsKgQa-2MDxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Establishing a baseline\n",
        "Besides splitting the data, we need a few more steps of data preparation before our machine learning algorithm can work.\n",
        "\n",
        "1. We can get rid of columns that are unlikely to contribute to the predictions: see `.drop()` below.\n",
        "2. We need to transform string data into categorical/numerical values (for the way some machine learning algorithms are optimized): see `pd.get_dummies()` below.\n",
        "\n",
        "We'll cover how to do this more reliably in Session 3; for now, here are quick and dirty ways to do these.\n"
      ],
      "metadata": {
        "id": "QoflgIg6A_qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.get_dummies(strat_train_set.drop([\"Survived\"], axis=1))\n",
        "y_train = strat_train_set[\"Survived\"]\n",
        "\n",
        "X_val = pd.get_dummies(strat_val_set.drop([\"Survived\"], axis=1))\n",
        "y_val = strat_val_set[\"Survived\"]\n",
        "\n",
        "display(X_train.head())"
      ],
      "metadata": {
        "id": "ZmbH5YHVBUBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a basic _decision tree_ (we'll cover this in Session 3) as our baseline to classify between `Survived=0` and `Survived=1`."
      ],
      "metadata": {
        "id": "JwsKifUty49d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "acc = clf.score(X_val, y_val)\n",
        "\n",
        "print(f\"Your {clf.__class__.__name__} predicts 'Survived'\")\n",
        "print(f\" with an validation set accuracy of {acc*100:.2f}\")"
      ],
      "metadata": {
        "id": "RXgy2AFq5DJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You've trained and run your first ML algorithm!"
      ],
      "metadata": {
        "id": "_ub8z8q4NGpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2.2(b): Evaluation practice\n",
        "Now that we have a working ML classifier, let's look at the evaluation environment.\n",
        "\n",
        "* ML algorithms often give different answers even with the same parameters. Write a loop that trains 5 classifiers and averages their scores. (Hint: vary or remove `random_state`.)\n",
        "* ML algorithms have lots of options. Today, we're not focusing on what those options mean, but on how to test between them. Write a loop or other function that tests out the hyperparameters (options) for [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier): `criterion`, `splitter`, and `max_features`. Which values for each option give the best (averaged over 5) results?"
      ],
      "metadata": {
        "id": "lp5vDI1SEDOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra: Cross-validation\n",
        "Cross-validation on a training set can be very helpful for finding the best values. In `sklearn` you can do this with less code.\n",
        "\n",
        "Use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and/or [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) to find the best values for hyperparameters."
      ],
      "metadata": {
        "id": "h26a2VvGPGQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SESSION 3: REGRESSION AND CLASSIFICATION\n",
        "After completing these exercises , you should be able to:\n",
        "- Implement data pipelines for streamlined ML-related preprocessing.\n",
        "- Scale features with `StandardScaler`.\n",
        "- Train a supervised machine learning algorithm with hyperparameter tuning.\n",
        "- Make predictions and save results."
      ],
      "metadata": {
        "id": "Fhqei5lclvQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3.1: An ML Training Pipeline\n",
        "In this exercise, we'll continue using the simple Titanic dataset to do basic *classification*, but we'll use the `sklearn` package's ecosystem of \"estimators,\" \"transformers,\" and pipelines to make the task robust."
      ],
      "metadata": {
        "id": "c6f2QukZADRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "psKlOsH5mPcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('titanicData/train.csv')\n",
        "test_data = pd.read_csv('titanicData/test.csv')\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2)\n",
        "for train_indices, test_indicices in split.split( data, data[[\"Survived\" , \"Pclass\" , \"Sex\"]]):\n",
        "    strat_train_set = data.loc[train_indices]\n",
        "    strat_val_set = data.loc[test_indicices]"
      ],
      "metadata": {
        "id": "o3_LTbLMmU25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}